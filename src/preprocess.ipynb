{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG7HbXjDbWIk",
        "outputId": "8a310700-3130-4110-e704-ca088f74ebdc"
      },
      "outputs": [],
      "source": [
        "%pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b32WexQtbWxM",
        "outputId": "39b02658-9255-4f3d-f001-5ff439561465"
      },
      "outputs": [],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3gr1pD3bY5I",
        "outputId": "4f92dc36-04cf-4c57-aa7a-1f7ba0523155"
      },
      "outputs": [],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIy70sDsr-s7",
        "outputId": "56a9b9b0-2168-4b4b-820a-fc78a1297a4e"
      },
      "outputs": [],
      "source": [
        "%pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Th1cA7GbbVY",
        "outputId": "d46a2a4d-e1db-459e-c10e-6a6e290f9752"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUiPZkOnbeEY"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/A_Progetti/AlgoForMassiveDatasets/kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7mVwO8fbhtM",
        "outputId": "477c0272-ea04-4f41-9df4-eacb01285a6d"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PcSKMiabQfo"
      },
      "source": [
        "# Preprocessing\n",
        "We're going to preprocess the original dataset from Kaggle to reduce its size and only work on meaningful data for our analysis: \n",
        "1. unzip the provided file to work on individual CSV files;\n",
        "2. filter only English written tweets to build a coherent language base;\n",
        "3. remove useless columns such as the account description or the number of retweets;\n",
        "4. possibly remove some stop-words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGloinN5cB-0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-nk-h98bQfo"
      },
      "source": [
        "## Code imports and globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKJIbm2HbQfo"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pyspark\n",
        "import regex\n",
        "import shutil\n",
        "import gzip\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "from pyspark.sql.types import Row\n",
        "from pyspark.sql.functions import concat_ws\n",
        "import multiprocessing\n",
        "import pandas\n",
        "from typing import List\n",
        "from datetime import datetime\n",
        "from nltk.corpus import stopwords\n",
        "import json\n",
        "\n",
        "\n",
        "KAGGLE_DATASET_DIRECTORY = \"out/ukraine-russian-crisis-twitter-dataset-1-2-m-rows\"\n",
        "KAGGLE_DATASET = \"ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip\"\n",
        "WORKERS_CORES = multiprocessing.cpu_count()\n",
        "FILTER_LANGUAGE = \"en\"\n",
        "\n",
        "TOP_HASHTAGS = ['ukraine', 'russia', 'standwithukraine', 'putin', 'russian', 'mariupol', 'news', 'azovstal', 'ukrainerussianwar', 'ukrainian', 'nato', 'ukrainewar', 'ukraine', 'business', 'ukrainerussiawar', 'kharkiv', 'usa', 'tigray', 'armukrainenow', 'kyiv', 'russianukrainianwar', 'stoprussia', 'slavaukraini', 'standwithukraine', 'stopputinnow', 'stopputin', 'war', 'russiaukrainewar', 'savemariupol', 'zelenskyy', 'anonymous', 'biden', 'russians', 'eu', 'ukraineunderattack', 'kherson', 'standwithukriane', 'scotus', 'helpukraine', 'russianwarcrimes', 'tigraygenocide', 'us', 'nft', 'china', 'zelenskiynft', 'zelensky', 'endtigraysiege', 'safeairliftukraine', 'azov', 'giveaway', 'europe', 'ukraineunderattack', 'oprussia', 'donbass', 'canada', 'bucha', 'moscow', 'nfts', 'syria', 'poland', 'uk', 'russiaukraine', 'breaking', 'india', 'odesa', 'germany', 'ukrainians', 'russianarmy', 'donetsk', 'putinwarcrimes', 'mykolaiv', 'ukrainerussia', 'belarus', 'warcrimes', 'kiev', 'healthy', 'healthyeating', 'donbas', 'ukraineinvasion', 'breakingnews', 'roevwade', 'united24', 'luhansk', 'peace', 'stoprussianaggression', 'supportukraine', 'severodonetsk', 'putinwarcriminal', 'odessa', 'crypto', 'irpin', 'bitcoin', 'noflyzone', 'moskva', 'trump', 'closethesky', 'product', 'freeshipping', 'russiaukraineconflict', 'warinukraine']\n",
        "\n",
        "\n",
        "def update_top_hashtags():\n",
        "    global TOP_HASHTAGS_INDEX, TOP_HASHTAGS_REVERSE_INDEX\n",
        "    zipped_hashtags = list(zip(TOP_HASHTAGS, range(len(TOP_HASHTAGS))))\n",
        "    TOP_HASHTAGS_INDEX = {key: value for key, value in zipped_hashtags}\n",
        "    TOP_HASHTAGS_REVERSE_INDEX = {value: key for key, value in zipped_hashtags}\n",
        "\n",
        "TOP_HASHTAGS_INDEX = dict()\n",
        "TOP_HASHTAGS_REVERSE_INDEX = dict()\n",
        "update_top_hashtags()\n",
        "\n",
        "TOP_HASHTAGS_INDEX_FILENAME = \"top_hashtags_index.json\"\n",
        "TOP_HASHTAGS_REVERSE_INDEX_FILENAME = \"top_hashtags_reverse_index.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DO_PREPROCESS = True\n",
        "FIND_MOST_COMMON = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3JdK-o4dNyY"
      },
      "source": [
        "## Stopwords list retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjRKq3D-dQyk"
      },
      "outputs": [],
      "source": [
        "def update_nltk():\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dqpJE0hbQfo"
      },
      "source": [
        "## Dataset extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmXEZixNbQfo"
      },
      "outputs": [],
      "source": [
        "def dataset_extraction(archive: str, output_directory: str) -> List[str]:\n",
        "    new_files = list()\n",
        "    with zipfile.ZipFile(archive, \"r\") as zip_ref:\n",
        "\n",
        "        #return [os.path.join(output_directory, csv_name) for csv_name, _ in [os.path.splitext(g_name) for g_name in zip_ref.namelist()[:2]]]\n",
        "\n",
        "        if not os.path.isdir(output_directory):\n",
        "            zip_ref.extractall(output_directory)\n",
        "            #new_files = zip_ref.namelist()\n",
        "        # else:\n",
        "        for gzip_name in zip_ref.namelist():\n",
        "            csv_name, extension = os.path.splitext(gzip_name)\n",
        "            if os.path.isfile(os.path.join(output_directory, csv_name)):\n",
        "                continue\n",
        "            if not os.path.isfile(os.path.join(output_directory, gzip_name)):\n",
        "                zip_ref.extract(gzip_name, path=output_directory)\n",
        "            csv_path = os.path.join(output_directory, csv_name)\n",
        "            with gzip.open(os.path.join(output_directory, gzip_name), 'r') as gzip_file, open(csv_path, 'wb') as csv_file:\n",
        "                shutil.copyfileobj(gzip_file, csv_file)\n",
        "            new_files.append(csv_path)\n",
        "    return new_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38HBJGytbQfo"
      },
      "source": [
        "## Spark initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG9S6yf7bQfo"
      },
      "outputs": [],
      "source": [
        "def init_spark():\n",
        "    print(f\"Available CPU cores/workers: {WORKERS_CORES}\")\n",
        "    print(\"Initializing spark...\", end=' ', flush=True)\n",
        "    spark = (\n",
        "        pyspark.sql.SparkSession.builder\n",
        "        .master(f\"local[{WORKERS_CORES}]\")\n",
        "        .appName(\"Sparkiodi\")\n",
        "        .getOrCreate()\n",
        "    )\n",
        "    spark.sparkContext.setLogLevel(\"OFF\")\n",
        "    print(\"Spark initialized\")\n",
        "    return spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxVWcA51bQfo"
      },
      "source": [
        "## Dataset reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziq-GV6tbQfo"
      },
      "outputs": [],
      "source": [
        "def read_dataframe(path: List[str], spark, header: bool = True, language: str = None):\n",
        "    if header:\n",
        "        starting_df = (\n",
        "            spark\n",
        "            .read\n",
        "            .option(\"header\", True)\n",
        "            .option(\"multiLine\", True)\n",
        "            .csv(path)\n",
        "            # .csv(f\"{KAGGLE_DATASET_DIRECTORY}/*.csv\")  # Read all files\n",
        "            # .csv(f\"{KAGGLE_DATASET_DIRECTORY}/UkraineCombinedTweetsDeduped_FEB28_part2.csv\")    # Read the smallest file\n",
        "            # .csv(f\"{KAGGLE_DATASET_DIRECTORY}/0505_to_0507_UkraineCombinedTweetsDeduped.csv\")  # Read the biggest file\n",
        "        )\n",
        "    else:\n",
        "        starting_df = (\n",
        "            spark\n",
        "            .read\n",
        "            .option(\"header\", False)\n",
        "            .option(\"multiLine\", True)\n",
        "            .csv(path)\n",
        "            # .csv(f\"{KAGGLE_DATASET_DIRECTORY}/*.csv\")  # Read all files\n",
        "            # .csv(f\"{KAGGLE_DATASET_DIRECTORY}/UkraineCombinedTweetsDeduped_FEB28_part2.csv\")    # Read the smallest file\n",
        "            # .csv(f\"{KAGGLE_DATASET_DIRECTORY}/0505_to_0507_UkraineCombinedTweetsDeduped.csv\")  # Read the biggest file\n",
        "        )\n",
        "\n",
        "    if language:\n",
        "        broadcast_language = spark.sparkContext.broadcast(FILTER_LANGUAGE)\n",
        "        starting_df = starting_df.where(\n",
        "            starting_df.language == broadcast_language.value)\n",
        "\n",
        "    return starting_df.select(\"text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0DArgu1bQfo"
      },
      "source": [
        "### Entities removal and hashtags extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpABLRDBbQfo"
      },
      "outputs": [],
      "source": [
        "hashtag_regex_str = r\"(?:\\#+)([\\w_]+)\"  # hashtags\n",
        "regex_str = [\n",
        "    (r'(?:@[\\w_]+)', ''),  # @-mentions\n",
        "    (r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', ''), # URLs\n",
        "    (r'[^\\w\\s]', ' '),  # punctuation\n",
        "    (r'\\s+', ' ')  # whitespaces\n",
        "]\n",
        "\n",
        "hashtag_regex = regex.compile(hashtag_regex_str)\n",
        "regex = [(hashtag_regex, '')] + [(regex.compile(compiled[0]), compiled[1])\n",
        "                                 for compiled in regex_str]   # Keep hashtag_regex as the first applied regex\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "\n",
        "    for reg in regex:\n",
        "        text = reg[0].sub(reg[1], text)\n",
        "\n",
        "    text_list = set([\n",
        "        word.rstrip() for word in word_tokenize(text)\n",
        "        if word not in string.punctuation\n",
        "    ])\n",
        "\n",
        "    return list(text_list)\n",
        "\n",
        "\n",
        "def from_indices_to_boolean_vector(indices):\n",
        "    pads = [0 for _ in range(len(TOP_HASHTAGS))]\n",
        "    for index in indices:\n",
        "        pads[index] = 1\n",
        "    return pads\n",
        "\n",
        "\n",
        "def padding_hashtags(hashtags, indexes, compress=False):\n",
        "    if compress:\n",
        "        pads = list()\n",
        "        for tag in hashtags:\n",
        "          if tag in indexes:\n",
        "              pads.append(indexes[tag])\n",
        "        pads.sort()\n",
        "    else:\n",
        "        pads = [0 for _ in range(len(indexes))]\n",
        "        for tag in hashtags:\n",
        "            if tag in indexes:\n",
        "                pads[indexes[tag]] = 1\n",
        "    return pads\n",
        "\n",
        "\n",
        "def clean_dataframe(df, spark):\n",
        "    remover = StopWordsRemover(stopWords=stopwords.words('english'))\n",
        "    remover.setInputCol(\"tweet\")\n",
        "    remover.setOutputCol(\"filtered_tweet\")\n",
        "\n",
        "    tweets_hashtags_rdd = (\n",
        "        df.rdd\n",
        "        .map(lambda row: (clean_text(row.text), [ht.lower() for ht in hashtag_regex.findall(row.text)]))\n",
        "    )\n",
        "\n",
        "    if FIND_MOST_COMMON:\n",
        "        global TOP_HASHTAGS\n",
        "        TOP_HASHTAGS = (\n",
        "            tweets_hashtags_rdd\n",
        "            .flatMap(lambda row: row[1])\n",
        "            .map(lambda row: (row, 1))\n",
        "            .reduceByKey(lambda x, y: x + y)\n",
        "            .map(lambda row: (row[1], row[0]))\n",
        "            .sortByKey(ascending=False)\n",
        "            .map(lambda row: row[1])\n",
        "            .take(100)\n",
        "        )\n",
        "        update_top_hashtags()\n",
        "\n",
        "    broadcast_top_hashtags_index = spark.sparkContext.broadcast(TOP_HASHTAGS_INDEX)\n",
        "        \n",
        "    df = (\n",
        "        tweets_hashtags_rdd\n",
        "        .map(lambda row: (row[0], padding_hashtags(row[1], broadcast_top_hashtags_index.value, True)))\n",
        "        .toDF([\"tweet\", \"hashtags\"])\n",
        "    )\n",
        "    df = (\n",
        "        remover\n",
        "        .transform(df)\n",
        "        .select(\"filtered_tweet\", \"hashtags\")\n",
        "        .withColumn(\"filtered_tweet\", concat_ws(\" \", \"filtered_tweet\"))\n",
        "        .withColumn(\"hashtags\", concat_ws(\" \", \"hashtags\"))\n",
        "    )\n",
        "    df = df.where((df.filtered_tweet != \"\"))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH7ZHI4ObQfo"
      },
      "source": [
        "## Preprocessed dataset storing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0Hn0pWabQfo"
      },
      "outputs": [],
      "source": [
        "def write_preprocessed(df, output_directory: str) -> str:\n",
        "    print(\"Writing top hashtags...\")\n",
        "    with open(TOP_HASHTAGS_INDEX_FILENAME, \"w\") as fp:\n",
        "        json.dump(TOP_HASHTAGS_INDEX, fp)\n",
        "    with open(TOP_HASHTAGS_REVERSE_INDEX_FILENAME, \"w\") as fp:\n",
        "        json.dump(TOP_HASHTAGS_REVERSE_INDEX, fp)\n",
        "    print(\"Writing all the CSVs...\")\n",
        "    (df\n",
        "        # .coalesce(WORKERS_CORES)\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .option(\"header\", False)\n",
        "        .csv(output_directory)\n",
        "     )\n",
        "\n",
        "    for dirpath, dirnames, filenames in os.walk(output_directory):\n",
        "        for filename in filenames:\n",
        "            name, extension = os.path.splitext(filename)\n",
        "            if extension == \".csv\":\n",
        "                continue\n",
        "            filepath = os.path.join(dirpath, filename)\n",
        "            try:\n",
        "                os.remove(filepath)\n",
        "            except:\n",
        "                pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldEu38RibQfo"
      },
      "outputs": [],
      "source": [
        "def compress(path: str) -> str:\n",
        "    print(\"Creating archive directory...\")\n",
        "    if os.path.isfile(path):\n",
        "        name, ext = os.path.splitext(path)\n",
        "        output_archive = f\"{name}.zip\"\n",
        "        with zipfile.ZipFile(output_archive, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "            zf.write(path, os.path.basename(path))\n",
        "    else:\n",
        "        output_archive = shutil.make_archive(path, 'zip', path)\n",
        "    return output_archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Q9_1C7bQfo"
      },
      "outputs": [],
      "source": [
        "def create_single_csv(folders: List[str], overwrite: bool = False) -> str:\n",
        "    single_filename = 'out/dataset.csv'\n",
        "    filemode = 'wb' if overwrite else 'ab'\n",
        "    with open(single_filename, filemode) as output_file:\n",
        "        for directory in folders:\n",
        "            for dirpath, dirnames, filenames in os.walk(directory):\n",
        "                for filename in filenames:\n",
        "                    name, extension = os.path.splitext(filename)\n",
        "                    if extension != \".csv\":\n",
        "                        continue\n",
        "                    filepath = os.path.join(dirpath, filename)\n",
        "                    with open(filepath, 'rb') as csv:\n",
        "                        shutil.copyfileobj(csv, output_file)\n",
        "    return single_filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAT0ui32bQfo"
      },
      "source": [
        "## Start preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqFI4HprbQfo",
        "outputId": "d5cfe8ce-0d87-4e99-a97b-1f39a3ba2b6e"
      },
      "outputs": [],
      "source": [
        "def preprocess() -> None:\n",
        "    new_files = dataset_extraction(KAGGLE_DATASET, KAGGLE_DATASET_DIRECTORY)\n",
        "    if not len(new_files):\n",
        "        print(\"No new files to process\")\n",
        "        return\n",
        "\n",
        "    update_nltk()\n",
        "    if not ('spark' in locals() or 'spark' in globals()):\n",
        "        spark = init_spark()\n",
        "    df = read_dataframe(new_files, spark, language=FILTER_LANGUAGE)\n",
        "    df = clean_dataframe(df, spark)\n",
        "    output_directory = f\"out/preprocessed_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
        "    write_preprocessed(df, output_directory)\n",
        "    csv = create_single_csv([output_directory])\n",
        "    compress(csv)\n",
        "\n",
        "if DO_PREPROCESS:\n",
        "    preprocess()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfPWW6_UcGU8",
        "outputId": "d0873693-2cf9-4092-c426-2a7427642f7c"
      },
      "outputs": [],
      "source": [
        "end_time = time.time()\n",
        "print(f\"Start time: {start_time}\")\n",
        "print(f\"End time: {end_time}\")\n",
        "print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(end_time - start_time))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T13A7gZM6IGS",
        "outputId": "803ef720-ceb8-433a-91b1-c626342c1a81"
      },
      "outputs": [],
      "source": [
        "!head out/dataset.csv"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "preprocess.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c1b9ca2a5b5570164d24b37b8802acc778bb5029148cbff343ca7a6716f74867"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
