{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG7HbXjDbWIk",
        "outputId": "8a310700-3130-4110-e704-ca088f74ebdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle\n",
            "  Using cached kaggle-1.5.12-py3-none-any.whl\n",
            "Collecting python-slugify\n",
            "  Using cached python_slugify-6.1.2-py2.py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: python-dateutil in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from kaggle) (2.8.2)\n",
            "Collecting urllib3\n",
            "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "Collecting requests\n",
            "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "Collecting tqdm\n",
            "  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "Collecting certifi\n",
            "  Downloading certifi-2022.6.15-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.2/160.2 kB\u001b[0m \u001b[31m909.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from kaggle) (1.16.0)\n",
            "Collecting text-unidecode>=1.3\n",
            "  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
            "Collecting charset-normalizer<3,>=2\n",
            "  Downloading charset_normalizer-2.1.0-py3-none-any.whl (39 kB)\n",
            "Installing collected packages: text-unidecode, urllib3, tqdm, python-slugify, idna, charset-normalizer, certifi, requests, kaggle\n",
            "Successfully installed certifi-2022.6.15 charset-normalizer-2.1.0 idna-3.3 kaggle-1.5.12 python-slugify-6.1.2 requests-2.28.1 text-unidecode-1.3 tqdm-4.64.0 urllib3-1.26.9\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b32WexQtbWxM",
        "outputId": "39b02658-9255-4f3d-f001-5ff439561465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m667.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=375ade65bd0216c20e250cdbc46e7f2fb3e9c8efec9143ed018bc2baa62b094a\n",
            "  Stored in directory: /home/lion/.cache/pip/wheels/81/9c/6c/d5200fcf351ffa39cbe09911e99703283624cd037df58070d9\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3gr1pD3bY5I",
        "outputId": "4f92dc36-04cf-4c57-aa7a-1f7ba0523155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "Requirement already satisfied: tqdm in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from nltk) (4.64.0)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2022.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.0/764.0 kB\u001b[0m \u001b[31m701.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting joblib\n",
            "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "Collecting click\n",
            "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
            "Installing collected packages: regex, joblib, click, nltk\n",
            "Successfully installed click-8.1.3 joblib-1.1.0 nltk-3.7 regex-2022.6.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIy70sDsr-s7",
        "outputId": "56a9b9b0-2168-4b4b-820a-fc78a1297a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Downloading pandas-1.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
            "Collecting numpy>=1.21.0\n",
            "  Downloading numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pytz>=2020.1\n",
            "  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Installing collected packages: pytz, numpy, pandas\n",
            "Successfully installed numpy-1.23.0 pandas-1.4.3 pytz-2022.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUiPZkOnbeEY"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7mVwO8fbhtM",
        "outputId": "477c0272-ea04-4f41-9df4-eacb01285a6d"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PcSKMiabQfo"
      },
      "source": [
        "# Preprocessing\n",
        "We're going to preprocess the original dataset from Kaggle to reduce its size and only work on meaningful data for our analysis: \n",
        "1. unzip the provided file to work on individual CSV files;\n",
        "2. filter only English written tweets to build a coherent language base;\n",
        "3. remove useless columns such as the account description or the number of retweets;\n",
        "4. possibly remove some stop-words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGloinN5cB-0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-nk-h98bQfo"
      },
      "source": [
        "## Code imports and globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKJIbm2HbQfo"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pyspark\n",
        "import regex\n",
        "import shutil\n",
        "import gzip\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "from pyspark.sql.types import Row\n",
        "from pyspark.sql.functions import concat_ws\n",
        "import multiprocessing\n",
        "import pandas\n",
        "from typing import List, Dict\n",
        "from datetime import datetime\n",
        "from nltk.corpus import stopwords\n",
        "import json\n",
        "\n",
        "\n",
        "TOP_HASHTAGS = ['ukraine', 'russia', 'standwithukraine', 'putin', 'russian', 'mariupol', 'ukrainerussiawar', 'nato', 'tigray', 'ukrainewar', 'kyiv', 'ukrainian', 'stoprussia', 'news', 'armukrainenow', 'russiaukrainewar', 'usa', 'stopputin', 'kharkiv', 'slavaukraini', 'bucha', 'ukrainerussianwar', 'biden', 'ukraineunderattack', 'business', 'war', 'russianukrainianwar', 'stopputinnow', 'azovstal', 'eu', 'ukraine️', 'kherson', 'anonymous', 'standwithukraine️', 'russianwarcrimes', 'china', 'us', 'tigraygenocide', 'russians', 'zelensky', 'russianarmy', 'europe', 'endtigraysiege', 'nft', 'russiaukraine', 'belarus', 'breaking', 'ukrainerussia', 'ethiopia', 'poland', 'buchamassacre', 'ukraineunderattaсk', 'warcrimes', 'freeukraine', 'germany', 'zelenskyy', 'safeairliftukraine', 'donetsk', 'donbass', 'putinisawarcriminal', 'stopwar', 'ukrainians', 'moscow', 'kiev', 'ukraineinvasion', 'stoprussianaggression', 'putinwarcriminal', 'finland', 'trump', 'azov', 'uk', 'russiainvadedukraine', 'stopthewar', 'oprussia', 'canada', 'putinswar', 'russiaukraineconflict', 'putinwarcrimes', 'nfts', 'warinukraine', 'syria', 'india', 'peace', 'sweden', 'tigraycantwait', 'savemariupol', 'standupforukraine', 'humanitycomesfirst', 'helpukraine', 'supportukraine', 'odessa', 'ausgov', 'severodonetsk', 'eurovision', 'luhansk', 'nftcommunity', 'irpin', 'crypto', 'bitcoin', 'chernihiv']\n",
        "\n",
        "\n",
        "def update_top_hashtags():\n",
        "    global TOP_HASHTAGS_INDEX, TOP_HASHTAGS_REVERSE_INDEX\n",
        "    zipped_hashtags = list(zip(TOP_HASHTAGS, range(len(TOP_HASHTAGS))))\n",
        "    TOP_HASHTAGS_INDEX = {key: value for key, value in zipped_hashtags}\n",
        "    TOP_HASHTAGS_REVERSE_INDEX = {value: key for key, value in zipped_hashtags}\n",
        "\n",
        "TOP_HASHTAGS_INDEX = dict()\n",
        "TOP_HASHTAGS_REVERSE_INDEX = dict()\n",
        "update_top_hashtags()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DO_PREPROCESS = True\n",
        "FIND_MOST_COMMON = False\n",
        "\n",
        "TOP_HASHTAGS_INDEX_FILENAME = \"top_hashtags_index.json\"\n",
        "TOP_HASHTAGS_REVERSE_INDEX_FILENAME = \"top_hashtags_reverse_index.json\"\n",
        "\n",
        "KAGGLE_DATASET = \"ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip\"\n",
        "KAGGLE_DATASET_DIRECTORY = os.path.join(\"out\", os.path.splitext(KAGGLE_DATASET)[0])\n",
        "WORKERS_CORES = multiprocessing.cpu_count()\n",
        "FILTER_LANGUAGE = \"en\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3JdK-o4dNyY"
      },
      "source": [
        "## Stopwords list retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjRKq3D-dQyk"
      },
      "outputs": [],
      "source": [
        "def update_nltk() -> None:\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dqpJE0hbQfo"
      },
      "source": [
        "## Dataset extraction\n",
        "Unzip the kaggle dataset. If an extracted version of that dataset is already present, only extract new files. After that, extract .gzip files into .csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmXEZixNbQfo"
      },
      "outputs": [],
      "source": [
        "def dataset_extraction(archive: str, output_directory: str) -> List[str]:\n",
        "    new_files = list()\n",
        "    with zipfile.ZipFile(archive, \"r\") as zip_ref:\n",
        "\n",
        "        #return [os.path.join(output_directory, csv_name) for csv_name, _ in [os.path.splitext(g_name) for g_name in zip_ref.namelist()[:2]]]\n",
        "\n",
        "        if not os.path.isdir(output_directory):\n",
        "            zip_ref.extractall(output_directory)\n",
        "            #new_files = zip_ref.namelist()\n",
        "        # else:\n",
        "        for gzip_name in zip_ref.namelist():\n",
        "            csv_name, extension = os.path.splitext(gzip_name)\n",
        "            if os.path.isfile(os.path.join(output_directory, csv_name)):\n",
        "                continue\n",
        "            if not os.path.isfile(os.path.join(output_directory, gzip_name)):\n",
        "                zip_ref.extract(gzip_name, path=output_directory)\n",
        "            csv_path = os.path.join(output_directory, csv_name)\n",
        "            with gzip.open(os.path.join(output_directory, gzip_name), 'r') as gzip_file, open(csv_path, 'wb') as csv_file:\n",
        "                shutil.copyfileobj(gzip_file, csv_file)\n",
        "            new_files.append(csv_path)\n",
        "    return new_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38HBJGytbQfo"
      },
      "source": [
        "## Spark initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG9S6yf7bQfo"
      },
      "outputs": [],
      "source": [
        "def init_spark() -> pyspark.sql.SparkSession:\n",
        "    print(f\"Available CPU cores/workers: {WORKERS_CORES}\")\n",
        "    print(\"Initializing spark...\", end=' ', flush=True)\n",
        "    spark = (\n",
        "        pyspark.sql.SparkSession.builder\n",
        "        .master(f\"local[{WORKERS_CORES}]\")\n",
        "        .appName(\"Sparkiodi\")\n",
        "        .getOrCreate()\n",
        "    )\n",
        "    spark.sparkContext.setLogLevel(\"OFF\")\n",
        "    print(\"Spark initialized\")\n",
        "    return spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxVWcA51bQfo"
      },
      "source": [
        "## Dataset reduction\n",
        "Read all the CSVs in the specified path. \"path\" can be a list of files, a folder containig multiple files, or a regex matching multiple files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziq-GV6tbQfo"
      },
      "outputs": [],
      "source": [
        "def read_dataframe(path: List[str], spark, header: bool = True, language: str = None) -> pyspark.sql.DataFrame:\n",
        "    if header:\n",
        "        starting_df = (\n",
        "            spark\n",
        "            .read\n",
        "            .option(\"header\", True)\n",
        "            .option(\"multiLine\", True)\n",
        "            .csv(path)\n",
        "        )\n",
        "    else:\n",
        "        starting_df = (\n",
        "            spark\n",
        "            .read\n",
        "            .option(\"header\", False)\n",
        "            .option(\"multiLine\", True)\n",
        "            .csv(path)\n",
        "        )\n",
        "\n",
        "    if language:\n",
        "        broadcast_language = spark.sparkContext.broadcast(FILTER_LANGUAGE)\n",
        "        starting_df = starting_df.where(\n",
        "            starting_df.language == broadcast_language.value)\n",
        "\n",
        "    return starting_df.select(\"text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0DArgu1bQfo"
      },
      "source": [
        "### Entities removal and hashtags extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Regex\n",
        "Match hashtags, URLs, user-mentions, and punctuation, to eventually remove them from the tweet's text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hashtag_regex_str = r\"(?:\\#+)([\\w_]+)\"  # hashtags\n",
        "regex_str = [\n",
        "    (r'(?:@[\\w_]+)', ''),  # @-mentions\n",
        "    (r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', ''), # URLs\n",
        "    (r'[^\\w\\s]', ' '),  # punctuation\n",
        "    (r'\\s+', ' ')  # whitespaces\n",
        "]\n",
        "\n",
        "hashtag_regex = regex.compile(hashtag_regex_str)\n",
        "regex = [(hashtag_regex, '')] + [(regex.compile(compiled[0]), compiled[1])\n",
        "                                 for compiled in regex_str]   # Keep hashtag_regex as the first applied regex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Text cleaning\n",
        "Use the the regexes to filter out from the text anything that's not a meaningful word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "\n",
        "    for reg in regex:\n",
        "        text = reg[0].sub(reg[1], text)\n",
        "\n",
        "    text_list = set([\n",
        "        word.rstrip() for word in word_tokenize(text)\n",
        "        if word not in string.punctuation\n",
        "    ])\n",
        "\n",
        "    return list(text_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpABLRDBbQfo"
      },
      "outputs": [],
      "source": [
        "def padding_hashtags(hashtags: List[str], indexes: Dict[str, int], compress:Bool=False):\n",
        "    \"\"\" Utility function to create an indicator vector for the 'hashtags' list over the 'indexes' parameter\n",
        "    \n",
        "    Args:\n",
        "        hashtags (List[str]): list of hashtags to be mapped into an indicator vector\n",
        "        indexes (Dict[str, int]): mapping from an hashtag to its index position\n",
        "        compress (Bool): decides wether to store the whole indicator vector (False) or only the non-zero indices (True)\n",
        "    \n",
        "    Returns:\n",
        "        List[int]: a list of integer with the most common hashtags for the initial list\n",
        "    \"\"\"\n",
        "    if compress:\n",
        "        pads = list()\n",
        "        for tag in hashtags:\n",
        "          if tag in indexes:\n",
        "              pads.append(indexes[tag])\n",
        "        pads.sort()\n",
        "    else:\n",
        "        pads = [0 for _ in range(len(indexes))]\n",
        "        for tag in hashtags:\n",
        "            if tag in indexes:\n",
        "                pads[indexes[tag]] = 1\n",
        "    return pads\n",
        "\n",
        "\n",
        "def clean_dataframe(df:pyspark.sql.DataFrame, spark:pyspark.sql.SparkSession)-> pyspark.sql.DataFrame:\n",
        "\n",
        "    # StopWordsRemover is a pyspark utility to remove words from a dataset's column\n",
        "    remover = StopWordsRemover(stopWords=stopwords.words('english'))\n",
        "    remover.setInputCol(\"tweet\")\n",
        "    remover.setOutputCol(\"filtered_tweet\")\n",
        "\n",
        "    # From every row extract the \"cleaned\" text and a list of its hashtags\n",
        "    tweets_hashtags_rdd = (\n",
        "        df.rdd\n",
        "        .map(lambda row: (clean_text(row.text), [ht.lower() for ht in hashtag_regex.findall(row.text)]))\n",
        "    )\n",
        "\n",
        "    if FIND_MOST_COMMON:\n",
        "        global TOP_HASHTAGS\n",
        "        TOP_HASHTAGS = (\n",
        "            tweets_hashtags_rdd\n",
        "            .flatMap(lambda row: row[1])\n",
        "            .map(lambda row: (row, 1))\n",
        "            .reduceByKey(lambda x, y: x + y)\n",
        "            .map(lambda row: (row[1], row[0]))\n",
        "            .sortByKey(ascending=False)\n",
        "            .map(lambda row: row[1])\n",
        "            .take(100)\n",
        "        )\n",
        "        update_top_hashtags()\n",
        "\n",
        "    broadcast_top_hashtags_index = spark.sparkContext.broadcast(TOP_HASHTAGS_INDEX)\n",
        "\n",
        "    # Map the second column (containing the hashtags) into its indicator vector\n",
        "    df = (\n",
        "        tweets_hashtags_rdd\n",
        "        .map(lambda row: (row[0], padding_hashtags(row[1], broadcast_top_hashtags_index.value, True)))\n",
        "        .toDF([\"tweet\", \"hashtags\"])\n",
        "    )\n",
        "    # Remove the stop words and concatenate each column into a whitespace separated list\n",
        "    df = (\n",
        "        remover\n",
        "        .transform(df)\n",
        "        .select(\"filtered_tweet\", \"hashtags\")\n",
        "        .withColumn(\"filtered_tweet\", concat_ws(\" \", \"filtered_tweet\"))\n",
        "        .withColumn(\"hashtags\", concat_ws(\" \", \"hashtags\"))\n",
        "    )\n",
        "    # Filter out eventual tweets without text\n",
        "    df = df.where((df.filtered_tweet != \"\"))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH7ZHI4ObQfo"
      },
      "source": [
        "## Preprocessed dataset storing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0Hn0pWabQfo"
      },
      "outputs": [],
      "source": [
        "def write_preprocessed(df:pyspark.sql.DataFrame, output_directory: str) -> str:\n",
        "    print(\"Writing top hashtags...\")\n",
        "    with open(TOP_HASHTAGS_INDEX_FILENAME, \"w\") as fp:\n",
        "        json.dump(TOP_HASHTAGS_INDEX, fp)\n",
        "    with open(TOP_HASHTAGS_REVERSE_INDEX_FILENAME, \"w\") as fp:\n",
        "        json.dump(TOP_HASHTAGS_REVERSE_INDEX, fp)\n",
        "    print(\"Writing all the CSVs...\")\n",
        "    (df\n",
        "        # .coalesce(WORKERS_CORES)\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .option(\"header\", False)\n",
        "        .csv(output_directory)\n",
        "     )\n",
        "\n",
        "    # spark writes one file per task along with its CRC, so we delete all the non-CSV files we don't require\n",
        "    for dirpath, dirnames, filenames in os.walk(output_directory):\n",
        "        for filename in filenames:\n",
        "            name, extension = os.path.splitext(filename)\n",
        "            if extension == \".csv\":\n",
        "                continue\n",
        "            filepath = os.path.join(dirpath, filename)\n",
        "            try:\n",
        "                os.remove(filepath)\n",
        "            except:\n",
        "                pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldEu38RibQfo"
      },
      "outputs": [],
      "source": [
        "def compress(path: str) -> str:\n",
        "    print(\"Creating archive directory...\")\n",
        "    if os.path.isfile(path):\n",
        "        name, ext = os.path.splitext(path)\n",
        "        output_archive = f\"{name}.zip\"\n",
        "        with zipfile.ZipFile(output_archive, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "            zf.write(path, os.path.basename(path))\n",
        "    else:\n",
        "        output_archive = shutil.make_archive(path, 'zip', path)\n",
        "    return output_archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7Q9_1C7bQfo"
      },
      "outputs": [],
      "source": [
        "def create_single_csv(folders: List[str], overwrite: bool = False) -> str:\n",
        "    single_filename = 'out/dataset.csv'\n",
        "    filemode = 'wb' if overwrite else 'ab'\n",
        "    with open(single_filename, filemode) as output_file:\n",
        "        for directory in folders:\n",
        "            for dirpath, dirnames, filenames in os.walk(directory):\n",
        "                for filename in filenames:\n",
        "                    name, extension = os.path.splitext(filename)\n",
        "                    if extension != \".csv\":\n",
        "                        continue\n",
        "                    filepath = os.path.join(dirpath, filename)\n",
        "                    with open(filepath, 'rb') as csv:\n",
        "                        shutil.copyfileobj(csv, output_file)\n",
        "    return single_filename"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAT0ui32bQfo"
      },
      "source": [
        "## Start preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqFI4HprbQfo",
        "outputId": "d5cfe8ce-0d87-4e99-a97b-1f39a3ba2b6e"
      },
      "outputs": [],
      "source": [
        "def preprocess() -> None:\n",
        "    new_files = dataset_extraction(KAGGLE_DATASET, KAGGLE_DATASET_DIRECTORY)\n",
        "    if not len(new_files):\n",
        "        print(\"No new files to process\")\n",
        "        return\n",
        "\n",
        "    update_nltk()\n",
        "    if not ('spark' in locals() or 'spark' in globals()):\n",
        "        spark = init_spark()\n",
        "    df = read_dataframe(new_files, spark, language=FILTER_LANGUAGE)\n",
        "    df = clean_dataframe(df, spark)\n",
        "    output_directory = f\"out/preprocessed_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
        "    write_preprocessed(df, output_directory)\n",
        "    csv = create_single_csv([output_directory])\n",
        "    compress(csv)\n",
        "\n",
        "if DO_PREPROCESS:\n",
        "    preprocess()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfPWW6_UcGU8",
        "outputId": "d0873693-2cf9-4092-c426-2a7427642f7c"
      },
      "outputs": [],
      "source": [
        "end_time = time.time()\n",
        "print(f\"Start time: {start_time}\")\n",
        "print(f\"End time: {end_time}\")\n",
        "print(f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(end_time - start_time))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T13A7gZM6IGS",
        "outputId": "803ef720-ceb8-433a-91b1-c626342c1a81"
      },
      "outputs": [],
      "source": [
        "!head out/dataset.csv"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "preprocess.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 ('amd_venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "61e5296f38bf07bccdca0edcaafe3ad28c4d6c25ceaf884e8253fd5041a73447"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
