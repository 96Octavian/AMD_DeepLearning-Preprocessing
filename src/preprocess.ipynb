{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG7HbXjDbWIk",
        "outputId": "8a310700-3130-4110-e704-ca088f74ebdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from kaggle) (1.26.9)\n",
            "Requirement already satisfied: python-dateutil in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: requests in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from kaggle) (2.28.1)\n",
            "Requirement already satisfied: python-slugify in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: certifi in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from kaggle) (2022.6.15)\n",
            "Requirement already satisfied: six>=1.10 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from requests->kaggle) (2.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from requests->kaggle) (3.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b32WexQtbWxM",
        "outputId": "39b02658-9255-4f3d-f001-5ff439561465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (3.3.0)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from pyspark) (0.10.9.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3gr1pD3bY5I",
        "outputId": "4f92dc36-04cf-4c57-aa7a-1f7ba0523155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: joblib in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIy70sDsr-s7",
        "outputId": "56a9b9b0-2168-4b4b-820a-fc78a1297a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (1.4.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from pandas) (1.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/lion/Documents/unimi/algorithms_for_massive_datasets/AMD_DeepLearning/amd_venv/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tUiPZkOnbeEY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7mVwO8fbhtM",
        "outputId": "477c0272-ea04-4f41-9df4-eacb01285a6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PcSKMiabQfo"
      },
      "source": [
        "# Preprocessing\n",
        "We're going to preprocess the original dataset from Kaggle to reduce its size and only work on meaningful data for our analysis: \n",
        "1. unzip the provided file to work on individual CSV files;\n",
        "2. filter only English written tweets to build a coherent language base;\n",
        "3. remove useless columns such as the account description or the number of retweets;\n",
        "4. possibly remove some stop-words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FGloinN5cB-0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-nk-h98bQfo"
      },
      "source": [
        "## Code imports and globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QKJIbm2HbQfo"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pyspark\n",
        "import regex\n",
        "import shutil\n",
        "import gzip\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "from pyspark.sql.types import Row\n",
        "from pyspark.sql.functions import concat_ws\n",
        "import multiprocessing\n",
        "import pandas\n",
        "from typing import List, Dict\n",
        "from datetime import datetime\n",
        "from nltk.corpus import stopwords\n",
        "import json\n",
        "\n",
        "\n",
        "TOP_HASHTAGS_INDEX = dict()\n",
        "TOP_HASHTAGS_REVERSE_INDEX = dict()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "DO_PREPROCESS = True\n",
        "FIND_MOST_COMMON = True\n",
        "TOP_HASHTAGS_NUMBER = 100\n",
        "\n",
        "TOP_HASHTAGS_INDEX_FILENAME = \"top_hashtags_index.json\"\n",
        "TOP_HASHTAGS_REVERSE_INDEX_FILENAME = \"top_hashtags_reverse_index.json\"\n",
        "\n",
        "KAGGLE_DATASET = \"ukraine-russian-crisis-twitter-dataset-1-2-m-rows.zip\"\n",
        "KAGGLE_DATASET_DIRECTORY = os.path.join(\n",
        "    \"out\", os.path.splitext(KAGGLE_DATASET)[0])\n",
        "WORKERS_CORES = multiprocessing.cpu_count()\n",
        "FILTER_LANGUAGE = \"en\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3JdK-o4dNyY"
      },
      "source": [
        "## Stopwords list retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rjRKq3D-dQyk"
      },
      "outputs": [],
      "source": [
        "def update_nltk() -> None:\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dqpJE0hbQfo"
      },
      "source": [
        "## Dataset extraction\n",
        "Unzip the kaggle dataset. If an extracted version of that dataset is already present, only extract new files. After that, extract .gzip files into .csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kmXEZixNbQfo"
      },
      "outputs": [],
      "source": [
        "def dataset_extraction(archive: str, output_directory: str) -> List[str]:\n",
        "    new_files = list()\n",
        "    with zipfile.ZipFile(archive, \"r\") as zip_ref:\n",
        "\n",
        "        return [os.path.join(output_directory, csv_name) for csv_name, _ in [os.path.splitext(g_name) for g_name in zip_ref.namelist()]]\n",
        "\n",
        "        if not os.path.isdir(output_directory):\n",
        "            zip_ref.extractall(output_directory)\n",
        "            #new_files = zip_ref.namelist()\n",
        "        # else:\n",
        "        for gzip_name in zip_ref.namelist():\n",
        "            csv_name, extension = os.path.splitext(gzip_name)\n",
        "            if os.path.isfile(os.path.join(output_directory, csv_name)):\n",
        "                continue\n",
        "            if not os.path.isfile(os.path.join(output_directory, gzip_name)):\n",
        "                zip_ref.extract(gzip_name, path=output_directory)\n",
        "            csv_path = os.path.join(output_directory, csv_name)\n",
        "            with gzip.open(os.path.join(output_directory, gzip_name), 'r') as gzip_file, open(csv_path, 'wb') as csv_file:\n",
        "                shutil.copyfileobj(gzip_file, csv_file)\n",
        "            new_files.append(csv_path)\n",
        "    return new_files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38HBJGytbQfo"
      },
      "source": [
        "## Spark initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yG9S6yf7bQfo"
      },
      "outputs": [],
      "source": [
        "def init_spark() -> pyspark.sql.SparkSession:\n",
        "    print(f\"Available CPU cores/workers: {WORKERS_CORES}\")\n",
        "    print(\"Initializing spark...\", end=' ', flush=True)\n",
        "    spark = (\n",
        "        pyspark.sql.SparkSession.builder\n",
        "        .master(f\"local[{WORKERS_CORES}]\")\n",
        "        .appName(\"Sparkiodi\")\n",
        "        .getOrCreate()\n",
        "    )\n",
        "    spark.sparkContext.setLogLevel(\"OFF\")\n",
        "    print(\"Spark initialized\")\n",
        "    return spark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxVWcA51bQfo"
      },
      "source": [
        "## Dataset reduction\n",
        "Read all the CSVs in the specified path. \"path\" can be a list of files, a folder containig multiple files, or a regex matching multiple files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ziq-GV6tbQfo"
      },
      "outputs": [],
      "source": [
        "def read_dataframe(path: List[str], spark, header: bool = True, language: str = None) -> pyspark.sql.DataFrame:\n",
        "    if header:\n",
        "        starting_df = (\n",
        "            spark\n",
        "            .read\n",
        "            .option(\"header\", True)\n",
        "            .option(\"multiLine\", True)\n",
        "            .csv(path)\n",
        "        )\n",
        "    else:\n",
        "        starting_df = (\n",
        "            spark\n",
        "            .read\n",
        "            .option(\"header\", False)\n",
        "            .option(\"multiLine\", True)\n",
        "            .csv(path)\n",
        "        )\n",
        "\n",
        "    if language:\n",
        "        broadcast_language = spark.sparkContext.broadcast(FILTER_LANGUAGE)\n",
        "        starting_df = starting_df.where(\n",
        "            starting_df.language == broadcast_language.value)\n",
        "\n",
        "    return starting_df.select(\"text\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0DArgu1bQfo"
      },
      "source": [
        "### Entities removal and hashtags extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Regex\n",
        "Match hashtags, URLs, user-mentions, and punctuation, to eventually remove them from the tweet's text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "hashtag_regex_str = r\"(?:\\#+)([\\w_]+)\"  # hashtags\n",
        "regex_str = [\n",
        "    (r'(?:@[\\w_]+)', ''),  # @-mentions\n",
        "    # URLs\n",
        "    (r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', ''),\n",
        "    (r'[^\\w\\s]', ' '),  # punctuation\n",
        "    (r'\\s+', ' ')  # whitespaces\n",
        "]\n",
        "\n",
        "hashtag_regex = regex.compile(hashtag_regex_str)\n",
        "regex = [(hashtag_regex, '')] + [(regex.compile(compiled[0]), compiled[1])\n",
        "                                 for compiled in regex_str]   # Keep hashtag_regex as the first applied regex\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Text cleaning\n",
        "Use the the regexes to filter out from the text anything that's not a meaningful word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "\n",
        "    for reg in regex:\n",
        "        text = reg[0].sub(reg[1], text)\n",
        "\n",
        "    text_list = set([\n",
        "        word.rstrip() for word in word_tokenize(text)\n",
        "        if word not in string.punctuation\n",
        "    ])\n",
        "\n",
        "    return list(text_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hpABLRDBbQfo"
      },
      "outputs": [],
      "source": [
        "def padding_hashtags(hashtags: List[str], indexes: Dict[str, int], compress: bool = False):\n",
        "    \"\"\" Utility function to create an indicator vector for the 'hashtags' list over the 'indexes' parameter\n",
        "\n",
        "    Args:\n",
        "        hashtags (List[str]): list of hashtags to be mapped into an indicator vector\n",
        "        indexes (Dict[str, int]): mapping from an hashtag to its index position\n",
        "        compress (bool): decides wether to store the whole indicator vector (False) or only the non-zero indices (True)\n",
        "\n",
        "    Returns:\n",
        "        List[int]: a list of integer with the most common hashtags for the initial list\n",
        "    \"\"\"\n",
        "    if compress:\n",
        "        pads = list()\n",
        "        for tag in hashtags:\n",
        "            if tag in indexes:\n",
        "                pads.append(indexes[tag])\n",
        "        pads.sort()\n",
        "    else:\n",
        "        pads = [0 for _ in range(len(indexes))]\n",
        "        for tag in hashtags:\n",
        "            if tag in indexes:\n",
        "                pads[indexes[tag]] = 1\n",
        "    return pads\n",
        "\n",
        "\n",
        "def update_top_hashtags(top_hashtags: List[str]) -> None:\n",
        "    \"\"\" Utility function to update the global list of most common hashtags\n",
        "\n",
        "    Args:\n",
        "        top_hashtags (List[str]): list of most common hashtagsices (True)\n",
        "    \"\"\"\n",
        "    global TOP_HASHTAGS_INDEX, TOP_HASHTAGS_REVERSE_INDEX\n",
        "    zipped_hashtags = list(zip(top_hashtags, range(len(top_hashtags))))\n",
        "    TOP_HASHTAGS_INDEX = {key: value for key, value in zipped_hashtags}\n",
        "    TOP_HASHTAGS_REVERSE_INDEX = {value: key for key, value in zipped_hashtags}\n",
        "\n",
        "\n",
        "def clean_dataframe(df: pyspark.sql.DataFrame, spark: pyspark.sql.SparkSession) -> pyspark.sql.DataFrame:\n",
        "\n",
        "    # StopWordsRemover is a pyspark utility to remove words from a dataset's column\n",
        "    remover = StopWordsRemover(stopWords=stopwords.words('english'))\n",
        "    remover.setInputCol(\"tweet\")\n",
        "    remover.setOutputCol(\"filtered_tweet\")\n",
        "\n",
        "    # From every row extract the \"cleaned\" text and a list of its hashtags\n",
        "    tweets_hashtags_rdd = (\n",
        "        df.rdd\n",
        "        .map(lambda row: (clean_text(row.text), [ht.lower() for ht in hashtag_regex.findall(row.text)]))\n",
        "    )\n",
        "\n",
        "    if FIND_MOST_COMMON:\n",
        "        top_hashtags = (\n",
        "            tweets_hashtags_rdd\n",
        "            .flatMap(lambda row: row[1])\n",
        "            .map(lambda row: (row, 1))\n",
        "            .reduceByKey(lambda x, y: x + y)\n",
        "            .map(lambda row: (row[1], row[0]))\n",
        "            .sortByKey(ascending=False)\n",
        "            .map(lambda row: row[1])\n",
        "            .take(TOP_HASHTAGS_NUMBER)\n",
        "        )\n",
        "        update_top_hashtags(top_hashtags)\n",
        "\n",
        "    broadcast_top_hashtags_index = spark.sparkContext.broadcast(\n",
        "        TOP_HASHTAGS_INDEX)\n",
        "\n",
        "    # Map the second column (containing the hashtags) into its indicator vector\n",
        "    df = (\n",
        "        tweets_hashtags_rdd\n",
        "        .map(lambda row: (row[0], padding_hashtags(row[1], broadcast_top_hashtags_index.value, True)))\n",
        "        .toDF([\"tweet\", \"hashtags\"])\n",
        "    )\n",
        "    # Remove the stop words and concatenate each column into a whitespace separated list\n",
        "    df = (\n",
        "        remover\n",
        "        .transform(df)\n",
        "        .select(\"filtered_tweet\", \"hashtags\")\n",
        "        .withColumn(\"filtered_tweet\", concat_ws(\" \", \"filtered_tweet\"))\n",
        "        .withColumn(\"hashtags\", concat_ws(\" \", \"hashtags\"))\n",
        "    )\n",
        "    # Filter out eventual tweets without text\n",
        "    df = df.where((df.filtered_tweet != \"\"))\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH7ZHI4ObQfo"
      },
      "source": [
        "## Preprocessed dataset storing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "D0Hn0pWabQfo"
      },
      "outputs": [],
      "source": [
        "def write_preprocessed(df: pyspark.sql.DataFrame, output_directory: str) -> str:\n",
        "    print(\"Writing top hashtags...\")\n",
        "    with open(TOP_HASHTAGS_INDEX_FILENAME, \"w\") as fp:\n",
        "        json.dump(TOP_HASHTAGS_INDEX, fp)\n",
        "    with open(TOP_HASHTAGS_REVERSE_INDEX_FILENAME, \"w\") as fp:\n",
        "        json.dump(TOP_HASHTAGS_REVERSE_INDEX, fp)\n",
        "    print(\"Writing all the CSVs...\")\n",
        "    (df\n",
        "        # .coalesce(WORKERS_CORES)\n",
        "        .write\n",
        "        .mode(\"overwrite\")\n",
        "        .option(\"header\", False)\n",
        "        .csv(output_directory)\n",
        "     )\n",
        "\n",
        "    # spark writes one file per task along with its CRC, so we delete all the non-CSV files we don't require\n",
        "    for dirpath, dirnames, filenames in os.walk(output_directory):\n",
        "        for filename in filenames:\n",
        "            name, extension = os.path.splitext(filename)\n",
        "            if extension == \".csv\":\n",
        "                continue\n",
        "            filepath = os.path.join(dirpath, filename)\n",
        "            try:\n",
        "                os.remove(filepath)\n",
        "            except:\n",
        "                pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ldEu38RibQfo"
      },
      "outputs": [],
      "source": [
        "def compress(path: str) -> str:\n",
        "    print(\"Creating archive directory...\")\n",
        "    if os.path.isfile(path):\n",
        "        name, ext = os.path.splitext(path)\n",
        "        output_archive = f\"{name}.zip\"\n",
        "        with zipfile.ZipFile(output_archive, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "            zf.write(path, os.path.basename(path))\n",
        "    else:\n",
        "        output_archive = shutil.make_archive(path, 'zip', path)\n",
        "    return output_archive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "y7Q9_1C7bQfo"
      },
      "outputs": [],
      "source": [
        "def create_single_csv(folders: List[str], overwrite: bool = False) -> str:\n",
        "    single_filename = 'out/dataset.csv'\n",
        "    filemode = 'wb' if overwrite else 'ab'\n",
        "    with open(single_filename, filemode) as output_file:\n",
        "        for directory in folders:\n",
        "            for dirpath, dirnames, filenames in os.walk(directory):\n",
        "                for filename in filenames:\n",
        "                    name, extension = os.path.splitext(filename)\n",
        "                    if extension != \".csv\":\n",
        "                        continue\n",
        "                    filepath = os.path.join(dirpath, filename)\n",
        "                    with open(filepath, 'rb') as csv:\n",
        "                        shutil.copyfileobj(csv, output_file)\n",
        "    return single_filename\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAT0ui32bQfo"
      },
      "source": [
        "## Start preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqFI4HprbQfo",
        "outputId": "d5cfe8ce-0d87-4e99-a97b-1f39a3ba2b6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/lion/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available CPU cores/workers: 4\n",
            "Initializing spark... "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/lion/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/07/05 16:31:19 WARN Utils: Your hostname, elros resolves to a loopback address: 127.0.1.1; using 192.168.1.29 instead (on interface wlan0)\n",
            "22/07/05 16:31:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/07/05 16:31:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Spark initialized\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing top hashtags...\n",
            "Writing all the CSVs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating archive directory...\n"
          ]
        }
      ],
      "source": [
        "def preprocess() -> None:\n",
        "    if not FIND_MOST_COMMON:\n",
        "        if not os.path.isfile(TOP_HASHTAGS_INDEX_FILENAME):\n",
        "            print(f\"The flag \\\"FIND_MOST_COMMON\\\" is not set but no file at \\\"{TOP_HASHTAGS_INDEX_FILENAME}\\\" could be found.\")\n",
        "            print(\"Please either provide a file with the most common hashtags to be used or set \\\"FIND_MOST_COMMON\\\" to \\\"True\\\" to automatically compute them (note that this may take considerable time\")\n",
        "            return\n",
        "        else:\n",
        "            global TOP_HASHTAGS_INDEX, TOP_HASHTAGS_REVERSE_INDEX\n",
        "            with open(TOP_HASHTAGS_INDEX_FILENAME, \"r\") as input_file:\n",
        "                TOP_HASHTAGS_INDEX = json.load(input_file)\n",
        "                TOP_HASHTAGS_REVERSE_INDEX = {TOP_HASHTAGS_INDEX[item]: item for item in TOP_HASHTAGS_INDEX}\n",
        "                \n",
        "    new_files = dataset_extraction(KAGGLE_DATASET, KAGGLE_DATASET_DIRECTORY)\n",
        "    if not len(new_files):\n",
        "        print(\"No new files to process\")\n",
        "        return\n",
        "\n",
        "    update_nltk()\n",
        "    if not ('spark' in locals() or 'spark' in globals()):\n",
        "        spark = init_spark()\n",
        "    df = read_dataframe(new_files, spark, language=FILTER_LANGUAGE)\n",
        "    df = clean_dataframe(df, spark)\n",
        "    output_directory = f\"out/preprocessed_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
        "    write_preprocessed(df, output_directory)\n",
        "    csv = create_single_csv([output_directory])\n",
        "    compress(csv)\n",
        "\n",
        "\n",
        "if DO_PREPROCESS:\n",
        "    preprocess()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfPWW6_UcGU8",
        "outputId": "d0873693-2cf9-4092-c426-2a7427642f7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start time: 1657030413.5441806\n",
            "End time: 1657041867.3013732\n",
            "Elapsed time: 03:10:53\n"
          ]
        }
      ],
      "source": [
        "end_time = time.time()\n",
        "print(f\"Start time: {start_time}\")\n",
        "print(f\"End time: {end_time}\")\n",
        "print(\n",
        "    f\"Elapsed time: {time.strftime('%H:%M:%S', time.gmtime(end_time - start_time))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T13A7gZM6IGS",
        "outputId": "803ef720-ceb8-433a-91b1-c626342c1a81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "surgery cardiology practical russian health viktor scientific center marina died cardiac lyashko region kalabina shelling reported medical anesthesiologist pediatric minister troops,0 10\n",
            "supplies defense city points thought mps fled helping fight together fun team expect resistance patrol day town spent stay organize people checking part,0 10\n",
            "lab international could verified crimes indiscriminate imagery law amount evidence satellite analyzed violations photos including attacks crisis amnesty war digital videos,0\n",
            "tuesday financial invasion front amid sanctions caused west residents queues stood imposed long uncertainty,0 1 62\n",
            "10 bulletproof antiwar ideas,32 73\n",
            "snoring next bunker ok sleeping seriously bloke underground,6 10\n",
            "better want hundreds choice make wise made die already,0 1 3\n",
            "warnings collective heeded warned forget government officially expect legion relentless mr forgive cyber war anonymous us,0 32\n",
            "invasion ukraine worries connected world reputational biggest russian brands financial,0\n",
            "flights ukrainian hundreds following invasion archipelago closed airspace look indian stranded ocean civilian tourists says,0 1\n"
          ]
        }
      ],
      "source": [
        "!head out/dataset.csv\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "preprocess.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 ('amd_venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "61e5296f38bf07bccdca0edcaafe3ad28c4d6c25ceaf884e8253fd5041a73447"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
